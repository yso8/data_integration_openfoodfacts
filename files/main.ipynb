{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication de notre projet d'int√©gration des donn√©es\n",
    "### ü•ò - Diff√©rents type de r√©gimes : \n",
    "<b>‚Ä¢ Foodmap :</b> Favorise certains aliments contenant des glucides capable de fermenter dans l'intestin. <br>\n",
    "\n",
    "<b>‚Ä¢ M√©diterran√©en :</b> Favorise l'√©quilibre alimentaire, pour privil√©gier la vari√©t√© des aliments. <br>\n",
    "\n",
    "<b>‚Ä¢ Dash (Dietary Approaches to Stopping Hypertension) :</b> Permet de combattre l'hypertension art√©rielle en modifiant les habitudes alimentaires.<br>\n",
    "\n",
    "<b>‚Ä¢ √Ä indice glyc√©mique bas :</b> R√©gule le taux de sucre dans le sang en fonction des aliments et deux leur indice glyc√©mique (IG).<br>\n",
    "\n",
    "<b>‚Ä¢ V√©g√©tarien/V√©g√©talisme :</b> Exclut toutes chaires animales / Exclut la consommation de tous produits d'origine animale.<br>\n",
    "\n",
    "<b>‚Ä¢ Pal√©olithique :</b> Favorise la m√™me alimentation que nos anc√™tres, avant l'√®re industrielle. <br>\n",
    "\n",
    "<b>‚Ä¢ C√©tog√®ne :</b> Suppresion quasi compl√®te des sucres et amidons(glucides).<br>\n",
    "\n",
    "### üëî - √âtapes d'avancements \n",
    "<b>‚Ä¢ √âtape 1 :</b> Importation des libraries et des donn√©es<br>\n",
    "\n",
    "<b>‚Ä¢ √âtape 2 :</b> Nettoyage des donn√©es import√©es<br>\n",
    "\n",
    "<b>‚Ä¢ √âtape 3 :</b> G√©n√©ration des menus, en fonction des r√©gimes alimentaires des utilisateurs<br>\n",
    "\n",
    "<b>‚Ä¢ √âtape 4 :</b> Extraction des donn√©es vers la base de donn√©es<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>√âtape 1</b> : Importation des libraries et des donn√©es\n",
    "<hr/>\n",
    "\n",
    "### üìö - Importation de les libraries n√©cessaires pour le code \n",
    "\n",
    "Les imports de PySpark, notamment SparkSession pour la gestion de la session et les fonctions col et expr pour la manipulation des DataFrames, ont √©t√© r√©alis√©s pour permettre des op√©rations efficaces sur les donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import n√©cessaires des packages \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, expr\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíΩ - Importation de l'ensemble des donn√©es \n",
    "\n",
    "L'ensemble des donn√©es a √©t√© import√© et stock√© dans des DataFrames distincts, facilitant ainsi la manipulation et l'analyse ult√©rieure des informations relatives aux produits alimentaires et aux pays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'une session Spark, entr√©e principale pour int√©ragir avec pyspark\n",
    "spark = SparkSession.builder.appName(\"data_integration_openfoodfacts\").getOrCreate()\n",
    "\n",
    "# Importation des donn√©es openfoodfacts dans le dataframe, d√©limitation sur la tabulation et r√©cup√©ration de l'en-t√™te \n",
    "df_openfoodfacts = spark.read.option(\"delimiter\", \"\\t\").csv(\"../data/en.openfoodfacts.org.products.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# S√©lectionne seulement les colonnes qui nous int√©ressent pour la suite \n",
    "df_openfoodfacts = df_openfoodfacts.select([\"product_name\", \"brands\", \"categories\", \"countries\", \"energy-kcal_100g\", \"proteins_100g\", \"fat_100g\", \"carbohydrates_100g\"])\n",
    "\n",
    "# Importation des donn√©es de r√©gimes \n",
    "df_diets = spark.read.csv(\"../data/diets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Importation des donn√©es des utilisateurs \n",
    "df_users = spark.read.csv(\"../data/users.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Importation des donn√©es de tous les pays du mondes, pour pouvoir avoir un r√©f√©rentiel propre \n",
    "df_pays = spark.read.csv(\"../data/country.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Importation des donn√©es de jour, afin de d√©crire une semaine \n",
    "df_jours_semaine = spark.read.csv(\"../data/days.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Conserve seulement les colonnes qui nous int√®resse pour la suite des traitements \n",
    "df_pays = df_pays.select([\"id\", \"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä - Affichage du sch√©ma du DataFrame avec colonnes et types de donn√©es\n",
    "\n",
    "Le sch√©ma du DataFrame, affichant les colonnes ainsi que leurs types de donn√©es, a √©t√© affich√© pour offrir une vue d'ensemble des informations structur√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- brands: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- countries: string (nullable = true)\n",
      " |-- energy-kcal_100g: double (nullable = true)\n",
      " |-- proteins_100g: double (nullable = true)\n",
      " |-- fat_100g: double (nullable = true)\n",
      " |-- carbohydrates_100g: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affiche le sch√©ma du dataframe, indiquant les colonnes et leurs types de donn√©es \n",
    "df_openfoodfacts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>√âtape 2</b> : Nettoyage des donn√©es import√©es \n",
    "<hr/>\n",
    "\n",
    "### üóëÔ∏è - Suppression des valeurs nulles\n",
    "\n",
    "Dans cette section, nous avons supprim√© les lignes avec des valeurs nulles puis filtr√© les produits en veillant √† ce que des colonnes cl√©s ne contiennent pas de valeurs nulles, pr√©servant ainsi l'int√©grit√© des donn√©es. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes ayant des valeurs nulles \n",
    "df_openfoodfacts.dropna()\n",
    "\n",
    "# Filtrer les produits ayant des valeurs nulles \n",
    "df_openfoodfacts = df_openfoodfacts.filter(\n",
    "    col(\"product_name\").isNotNull() &\n",
    "    col(\"categories\").isNotNull() &\n",
    "    col(\"countries\").isNotNull() &\n",
    "    col(\"energy-kcal_100g\").isNotNull() &\n",
    "    col(\"proteins_100g\").isNotNull() &\n",
    "    col(\"fat_100g\").isNotNull() &\n",
    "    col(\"carbohydrates_100g\").isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóëÔ∏è - Suppresion des valeurs aberrantes\n",
    "\n",
    "Dans ce code, nous avons √©tabli des plages de valeurs minimales et maximales pour certaines donn√©es d'int√©r√™t. \n",
    "\n",
    "<b>√ânergie (kcal/100g)</b> : Les valeurs caloriques par portion devraient se situer entre 0 kcal/100g (minimum) et 800 kcal/100g (maximum) pour √™tre consid√©r√©es comme non aberrantes.\n",
    "\n",
    "<b>Prot√©ines (g/100g)</b> : La teneur en prot√©ines par portion serait consid√©r√©e comme non aberrante si elle se situe dans la plage de 1 g/100g (minimum) √† 100 g/100g (maximum).\n",
    "\n",
    "<b>Graisses (g/100g)</b> : Les quantit√©s de graisses par portion devraient √™tre comprises entre un minimum de 0 g/100g et un maximum de 100 g/100g pour √™tre consid√©r√©es comme non aberrantes.\n",
    "\n",
    "<b>Glucides (g/100g)</b> : La teneur en glucides par portion serait jug√©e non aberrante si elle se situe entre 1 g/100g (minimum) et 80 g/100g (maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir des valeurs sp√©cifiques pour chaque colonne\n",
    "valeurs_specifiques = {\n",
    "    'energy-kcal_100g': (0, 800),\n",
    "    'proteins_100g': (1, 100),\n",
    "    'fat_100g': (0, 100),\n",
    "    'carbohydrates_100g': (1, 80)\n",
    "}\n",
    "\n",
    "# Filtrer les produits qui contiennent des valeurs aberrantes pour les informations nutritionnelles\n",
    "df_openfoodfacts = df_openfoodfacts.filter(\n",
    "    (col(\"energy-kcal_100g\") > valeurs_specifiques['energy-kcal_100g'][0]) &\n",
    "    (col(\"energy-kcal_100g\") <= valeurs_specifiques['energy-kcal_100g'][1]) &\n",
    "    (col(\"proteins_100g\") > valeurs_specifiques['proteins_100g'][0]) &\n",
    "    (col(\"proteins_100g\") <= valeurs_specifiques['proteins_100g'][1]) &\n",
    "    (col(\"fat_100g\") > valeurs_specifiques['fat_100g'][0]) &\n",
    "    (col(\"fat_100g\") <= valeurs_specifiques['fat_100g'][1]) &\n",
    "    (col(\"carbohydrates_100g\") > valeurs_specifiques['carbohydrates_100g'][0]) &\n",
    "    (col(\"carbohydrates_100g\") <= valeurs_specifiques['carbohydrates_100g'][1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### üß∂ - Consolidation des donn√©es \n",
    "\n",
    "Dans ce bout de code, on fusionne les informations des produits alimentaires avec les donn√©es sp√©cifiques des pays via une jointure <b>\"left outer join\"</b>. \n",
    "\n",
    "Ensuite on s√©lectionne les colonnes pertinentes, filtre les produits sans correspondance dans les pays, et √©limine les doublons.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------------+-------------+--------+------------------+\n",
      "|        product_name|              brands|          categories|           countries|energy-kcal_100g|proteins_100g|fat_100g|carbohydrates_100g|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+-------------+--------+------------------+\n",
      "| Filet de merlu c...|    Tr√®s bien merci!|Plats pr√©par√©s, P...|              France|            68.0|          7.9|     2.1|               4.3|\n",
      "|&quot;CRISPY&quot...|              Melvit|N√∂v√©nyi alap√∫ √©le...|        Magyarorsz√°g|           457.0|         20.0|    29.0|              20.0|\n",
      "|&quot;La Traditio...|  Cr√™perie Le Masson|Cr√™pes et galette...|              France|           320.0|          7.0|     4.2|              64.0|\n",
      "|&quot;Les Mijot√©s...|                NULL|Viandes et d√©riv√©...|              France|           150.0|         21.0|     7.1|               1.1|\n",
      "|&quot;biscuit Mix...|                Gunz|Snacks, Snacks su...|              France|           492.0|          6.0|    22.3|              65.1|\n",
      "|'A Calamarata rigata|La fabbrica della...|Cibi e bevande a ...|               en:it|           350.0|         13.0|     1.0|              73.0|\n",
      "|              'Nduja|Terre d'Italia, C...|Prodotti a base d...|       France, italy|           604.0|          5.9|    60.0|               9.6|\n",
      "|  'Nduja di Spilinga|       Fattoria Sila|Prodotti a base d...|           en:France|           640.0|          6.0|    60.0|              19.0|\n",
      "|'e Matasse Taglia...|La fabbrica della...|Cibi e bevande a ...|               en:it|           350.0|         13.0|     1.0|              73.0|\n",
      "|'nana chips, blaz...|                NULL|              Snacks|France, United St...|           567.0|         3.33|   33.33|              50.0|\n",
      "|*Butter Spritzgeb...|Alnatura, Alnatur...|Imbiss, S√º√üer Sna...|         en:fr, Welt|           466.0|          5.9|    23.0|              58.0|\n",
      "|*Dinkel Mandel Sp...|Alnatura, Alnatur...|Imbiss, S√º√üer Sna...|Deutschland,Schwe...|           486.0|          7.4|    24.0|              59.0|\n",
      "|*Mini Schoko Lebk...|Alnatura,Bio, Aln...|Imbiss, S√º√üer Sna...|Frankreich,Deutsc...|           422.0|          8.0|    16.0|              58.0|\n",
      "|*Mini Vollmilch E...|Alnatura, Alnatur...|Kakao und Kakaopr...|       Germany, Welt|           561.0|          8.0|    37.0|              48.0|\n",
      "|*Mohn Marzipan Ko...|Alnatura, Alnatur...|Pflanzliche Leben...|Deutschland,Schwe...|           476.0|          7.7|    25.0|              53.0|\n",
      "|*Zartbitter Hase ...|Alnatura, Alnatur...|Imbiss, S√º√üer Sna...|     Allemagne, Welt|           587.0|          7.6|    45.0|              33.0|\n",
      "|   + Prote√≠nas Fresa| Hacendado,Schreiber|             L√°cteos|    Portugal,Espanha|            51.0|          7.1|     0.4|               4.8|\n",
      "|, Olives noires a...|Bouton d'Or,Inter...|Aliments et boiss...|              France|           349.0|          2.0|    34.6|               4.5|\n",
      "|          0% Chorizo|           Compasi√≥n|en:Meat alternati...|              Espa√±a|           409.0|         14.4|    31.4|              15.8|\n",
      "|0% FAT Greek styl...|Marks & Spencer,M...|Produits laitiers...|              France|           101.0|          4.8|     2.2|              14.9|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+-------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Effectuer une jointure left outer join\n",
    "joined_df = df_openfoodfacts.join(df_pays, df_openfoodfacts[\"countries\"] == df_pays[\"country\"], \"left_outer\")\n",
    "\n",
    "# S√©lectionner les colonnes qui nous int√©ressent et filtrer les lignes o√π il n'y a pas de correspondance dans df_pays\n",
    "df_openfoodfacts = joined_df.select([\"product_name\", \"brands\", \"categories\", \"countries\", \"energy-kcal_100g\", \"proteins_100g\", \"fat_100g\", \"carbohydrates_100g\"]).filter(col(\"countries\").isNotNull())\n",
    "\n",
    "# Supprimer les doublons\n",
    "df_openfoodfacts = df_openfoodfacts.dropDuplicates([\"product_name\", \"countries\"])\n",
    "\n",
    "# Afficher le dataframe \n",
    "df_openfoodfacts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>√âtape 3</b> : G√©n√©ration des menus, en fonction des r√©gimes alimentaires des utilisateurs\n",
    "<hr/>\n",
    "\n",
    " ### üç≤ - Cr√©ation des menus\n",
    "\n",
    "Dans ce bout de code, on g√©n√®re un petit-d√©jeuner, un d√©jeuner et un d√Æner pour chaque jour de la semaine par utilisateurs. En fonction de son r√©gime alimentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les r√©sultats\n",
    "weekly_menu = []\n",
    "\n",
    "# Boucle pour chaque utilisateur\n",
    "for utilisateur in df_users.collect():\n",
    "    # R√©cup√©rer les valeurs sp√©cifiques au r√©gime dans le DataFrame des r√©gimes\n",
    "    regime_utilisateur = df_diets.filter(col(\"Diet_type\") == utilisateur[\"diet\"]).first()\n",
    "    \n",
    "    # Boucle pour chaque jour de la semaine\n",
    "    for jour in df_jours_semaine.collect():\n",
    "        # Filtrer les produits disponibles qui respectent les valeurs du r√©gime alimentaire\n",
    "        filtered_products = df_openfoodfacts.filter(\n",
    "            (col(\"carbohydrates_100g\") <= regime_utilisateur[\"carbohydrates_100g\"]) &\n",
    "            (col(\"proteins_100g\") >= regime_utilisateur[\"proteins_100g\"]) &\n",
    "            (col(\"fat_100g\") <= regime_utilisateur[\"fat_100g\"]) &\n",
    "            (col(\"energy-kcal_100g\") <= regime_utilisateur[\"energy-kcal_100g\"])\n",
    "        )\n",
    "\n",
    "        # R√©cup√©ration des produits pour chaques repas de la journ√©e \n",
    "        breakfast = filtered_products.sample(withReplacement=False, fraction=0.1).first()\n",
    "        lunch = filtered_products.sample(withReplacement=False, fraction=0.1).first()\n",
    "        dinner = filtered_products.sample(withReplacement=False, fraction=0.1).first()\n",
    "\n",
    "            # Cr√©ation de l'objet r√©cup√©r√© par la suite contenant l'ensemble des informations \n",
    "        daily_menu = {\n",
    "            \"id_user\": utilisateur[\"id\"],\n",
    "            \"day\": jour[\"day\"],\n",
    "            \"breakfast\": breakfast.product_name,\n",
    "            \"lunch\": lunch.product_name,\n",
    "            \"dinner\": dinner.product_name\n",
    "        }\n",
    "\n",
    "        # Ajoute le menu dans la liste des menus globales \n",
    "        weekly_menu.append(daily_menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>√âtape 4</b> : Insertion des menus dans la base de donn√©es, √©tape d'extraction\n",
    "<hr/>\n",
    "\n",
    " ### üìâ - Extraction des donn√©es \n",
    "\n",
    "Ici, on extrait les donn√©es dans la base de donn√©es, on se connecte on ins√®re puis d√©connexion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion √† la base de donn√©es sqlite3\n",
    "conn = sqlite3.connect('../weekly_menu.db')\n",
    "\n",
    "# Cr√©ation d'un curseur\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# D√©finir la commande SQL d'insertion\n",
    "insert_command = '''\n",
    "INSERT INTO daily_menu (id_user, day, breakfast, lunch, dinner)\n",
    "VALUES (?, ?, ?, ?, ?);\n",
    "'''\n",
    "\n",
    "# Utiliser une transaction pour regrouper les insertions\n",
    "try:\n",
    "    for menu in weekly_menu:\n",
    "        # Ex√©cuter la commande SQL avec les donn√©es √† ins√©rer \n",
    "        cursor.execute(insert_command, (menu['id_user'], menu['day'], menu['breakfast'], menu['lunch'], menu['dinner']))\n",
    "\n",
    "    # Commiter les changements √† la fin de la transaction\n",
    "    conn.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    # En cas d'erreur, annuler la transaction\n",
    "    conn.rollback()\n",
    "    print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "finally:\n",
    "    # Fermeture du curseur et de la connexion √† la base de donn√©es \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### üîö - Fin des traitements\n",
    "\n",
    "Ensuite, on clos la session spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arr√™te la session Spark \n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
